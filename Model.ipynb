{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('df_train')\n",
    "df_test = pd.read_pickle('df_test')\n",
    "del(df_train['DateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = {'W': 1.5e5, 'HiT': 5, 'LoT' : 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['W'] = df_train['W'] / std['W']\n",
    "for param in ['HiT', 'LoT']:\n",
    "    df_train[param] = df_train[param] / std[param]\n",
    "    df_test[param] = df_test[param] / std[param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W</th>\n",
       "      <th>HiT</th>\n",
       "      <th>LoT</th>\n",
       "      <th>isWeekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.463985</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.322316</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.228695</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.109198</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.237810</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.122625</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.610293</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.879477</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.389768</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.812751</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.289655</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.141343</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.526908</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.151816</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.930735</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.219530</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.863530</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.714960</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.559728</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.334817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.184715</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.044635</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.935146</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.622731</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.440633</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.274538</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.185594</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.118055</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.154375</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.052466</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>6.943750</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>6.545413</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>6.307789</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>6.215786</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>6.272914</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>5.965230</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>5.654907</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>5.218493</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>4.921540</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>4.909021</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>4.759489</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>4.949052</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>4.771395</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>4.582617</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>5.599603</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>6.256426</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1834</th>\n",
       "      <td>6.733124</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>6.708533</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>6.695493</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>6.419834</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1838</th>\n",
       "      <td>6.024301</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>5.988680</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1840</th>\n",
       "      <td>5.862726</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>6.210493</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>6.526997</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>6.586719</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>6.262180</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845</th>\n",
       "      <td>6.196034</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>6.388007</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>6.171583</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1848 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             W  HiT  LoT  isWeekend\n",
       "0     3.463985  6.0  3.6          0\n",
       "1     3.322316  6.0  3.6          0\n",
       "2     3.228695  6.0  3.6          0\n",
       "3     3.109198  6.0  3.6          0\n",
       "4     3.237810  6.0  3.6          0\n",
       "5     3.122625  6.0  3.6          0\n",
       "6     2.610293  6.0  3.6          0\n",
       "7     2.879477  6.0  3.6          0\n",
       "8     3.389768  6.0  3.6          0\n",
       "9     4.812751  6.0  3.6          0\n",
       "10    5.289655  6.0  3.6          0\n",
       "11    5.141343  6.0  3.6          0\n",
       "12    5.526908  6.0  3.6          0\n",
       "13    5.151816  6.0  3.6          0\n",
       "14    4.930735  6.0  3.6          0\n",
       "15    5.219530  6.0  3.6          0\n",
       "16    4.863530  6.0  3.6          0\n",
       "17    4.714960  6.0  3.6          0\n",
       "18    4.559728  6.0  3.6          0\n",
       "19    4.334817  6.0  3.6          0\n",
       "20    4.184715  6.0  3.6          0\n",
       "21    4.044635  6.0  3.6          0\n",
       "22    3.935146  6.0  3.6          0\n",
       "23    3.622731  6.0  3.6          0\n",
       "24    3.440633  6.4  3.6          0\n",
       "25    3.274538  6.4  3.6          0\n",
       "26    3.185594  6.4  3.6          0\n",
       "27    3.118055  6.4  3.6          0\n",
       "28    3.154375  6.4  3.6          0\n",
       "29    3.052466  6.4  3.6          0\n",
       "...        ...  ...  ...        ...\n",
       "1818  6.943750  3.8  1.8          0\n",
       "1819  6.545413  3.8  1.8          0\n",
       "1820  6.307789  3.8  1.8          0\n",
       "1821  6.215786  3.8  1.8          0\n",
       "1822  6.272914  3.8  1.8          0\n",
       "1823  5.965230  3.8  1.8          0\n",
       "1824  5.654907  3.4  1.6          0\n",
       "1825  5.218493  3.4  1.6          0\n",
       "1826  4.921540  3.4  1.6          0\n",
       "1827  4.909021  3.4  1.6          0\n",
       "1828  4.759489  3.4  1.6          0\n",
       "1829  4.949052  3.4  1.6          0\n",
       "1830  4.771395  3.4  1.6          0\n",
       "1831  4.582617  3.4  1.6          0\n",
       "1832  5.599603  3.4  1.6          0\n",
       "1833  6.256426  3.4  1.6          0\n",
       "1834  6.733124  3.4  1.6          0\n",
       "1835  6.708533  3.4  1.6          0\n",
       "1836  6.695493  3.4  1.6          0\n",
       "1837  6.419834  3.4  1.6          0\n",
       "1838  6.024301  3.4  1.6          0\n",
       "1839  5.988680  3.4  1.6          0\n",
       "1840  5.862726  3.4  1.6          0\n",
       "1841  6.210493  3.4  1.6          0\n",
       "1842  6.526997  3.4  1.6          0\n",
       "1843  6.586719  3.4  1.6          0\n",
       "1844  6.262180  3.4  1.6          0\n",
       "1845  6.196034  3.4  1.6          0\n",
       "1846  6.388007  3.4  1.6          0\n",
       "1847  6.171583  3.4  1.6          0\n",
       "\n",
       "[1848 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back):\n",
    "\t\ta = dataset[i:(i+look_back)]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, :, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = df_train.values.reshape((-1, 24, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 24, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_dataset(dataset, look_back=7)\n",
    "X = X.reshape((X.shape[0], -1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70, 168, 4), (70, 24))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import adagrad, adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(7 * 24, 4)))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 100)               42000     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 24)                1224      \n",
      "=================================================================\n",
      "Total params: 48,274\n",
      "Trainable params: 48,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='mean_squared_error', optimizer=adagrad(lr=0.01))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train = 65\n",
    "X_train = X[:65]\n",
    "y_train = y[:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3/3 [==============================] - 2s - loss: 16.9620     \n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 1s - loss: 15.3102     \n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 1s - loss: 13.7250     \n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 1s - loss: 12.1522     \n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 1s - loss: 10.6179     \n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 1s - loss: 9.1406     \n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 1s - loss: 7.7241     \n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 1s - loss: 6.5432     \n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 1s - loss: 5.5993     \n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 1s - loss: 4.8307     \n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 1s - loss: 4.1126     \n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 1s - loss: 3.4676     \n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 1s - loss: 2.8644     \n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 2s - loss: 2.3516     \n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 1s - loss: 1.9079     \n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 1s - loss: 1.5296     \n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 1s - loss: 1.2030     \n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 1s - loss: 0.9291     \n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 1s - loss: 0.7125     \n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 1s - loss: 0.5382     \n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 1s - loss: 0.4015     \n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 1s - loss: 0.2942     \n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 1s - loss: 0.2103     \n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 1s - loss: 0.1502     \n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 1s - loss: 0.1081     \n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 1s - loss: 0.0783     \n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 1s - loss: 0.0579     \n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 1s - loss: 0.0445     \n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 1s - loss: 0.0357     \n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 1s - loss: 0.0296     \n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 1s - loss: 0.0252     \n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 1s - loss: 0.0224     \n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 1s - loss: 0.0205     \n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 1s - loss: 0.0195     \n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 1s - loss: 0.0186     \n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 1s - loss: 0.0188     \n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 1s - loss: 0.0179     \n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 1s - loss: 0.0180     \n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 1s - loss: 0.0176     \n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 1s - loss: 0.0169     \n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 1s - loss: 0.0168     \n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 1s - loss: 0.0178     \n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 1s - loss: 0.0177     \n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 1s - loss: 0.0172     \n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 1s - loss: 0.0172     \n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 1s - loss: 0.0172     \n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 1s - loss: 0.0173     \n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 1s - loss: 0.0173     \n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 1s - loss: 0.0177     \n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 1s - loss: 0.0171     \n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 1s - loss: 0.0167     \n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 1s - loss: 0.0178     \n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 1s - loss: 0.0171     \n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 1s - loss: 0.0179     \n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 1s - loss: 0.0168     \n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 1s - loss: 0.0168     \n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 1s - loss: 0.0170     \n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 1s - loss: 0.0172     \n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 1s - loss: 0.0171     \n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 1s - loss: 0.0169     \n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 1s - loss: 0.0180     \n",
      "Epoch 62/200\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0228"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-292e80accf96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Muks/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/Muks/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/Users/Muks/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Muks/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Muks/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Muks/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Muks/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Muks/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Muks/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=200, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122be0fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_pred.reshape(24))\n",
    "plt.plot(y[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer=adagrad(lr=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3/3 [==============================] - 2s - loss: 393810859349.3333     \n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 2s - loss: 392974510762.6667     \n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 1s - loss: 392338953557.3333     \n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 1s - loss: 391781242197.3333     \n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 1s - loss: 391268248234.6667     \n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 1s - loss: 390783849813.3333     \n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 1s - loss: 390320051541.3333     \n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 1s - loss: 389871916373.3333     \n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 1s - loss: 389436506112.0000     \n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 1s - loss: 389011701760.0000     \n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 1s - loss: 388595635541.3333     \n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 1s - loss: 388187433642.6667     \n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 1s - loss: 387785129984.0000     \n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 1s - loss: 387388888405.3333     \n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 1s - loss: 386997736789.3333     \n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 1s - loss: 386610735786.6667     \n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 1s - loss: 386228344149.3333     \n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 1s - loss: 385849709909.3333     \n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 1s - loss: 385474461696.0000     \n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 1s - loss: 385102533973.3333     \n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 1s - loss: 384732943701.3333     \n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 1s - loss: 384365614421.3333     \n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 1s - loss: 384001376256.0000     \n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 1s - loss: 383639180629.3333     \n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 1s - loss: 383279497216.0000     \n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 1s - loss: 382921495893.3333     \n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 1s - loss: 382565930325.3333     \n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 1s - loss: 382212101461.3333     \n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 1s - loss: 381859812693.3333     \n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 1s - loss: 381509020330.6667     \n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 1s - loss: 381160095744.0000     \n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 1s - loss: 380812667562.6667     \n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 1s - loss: 380466429952.0000     \n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 1s - loss: 380121699669.3333     \n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 1s - loss: 379778225493.3333     \n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 1s - loss: 379435701589.3333     \n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 1s - loss: 379094543018.6667     \n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 1s - loss: 378754738858.6667     \n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 1s - loss: 378415874048.0000     \n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 1s - loss: 378078232576.0000     \n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 1s - loss: 377741497685.3333     \n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 1s - loss: 377405702144.0000     \n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 1s - loss: 377070627498.6667     \n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 1s - loss: 376736492202.6667     \n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 1s - loss: 376403405482.6667     \n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 1s - loss: 376071159808.0000     \n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 1s - loss: 375739886250.6667     \n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 1s - loss: 375409442816.0000     \n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 1s - loss: 375079654741.3333     \n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 1s - loss: 374750718634.6667     \n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 1s - loss: 374422547114.6667     \n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 1s - loss: 374095096490.6667     \n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 1s - loss: 373768323072.0000     \n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 1s - loss: 373442106709.3333     \n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 1s - loss: 373116512938.6667     \n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 1s - loss: 372791672832.0000     \n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 1s - loss: 372467466240.0000     \n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 1s - loss: 372143991466.6667     \n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 1s - loss: 371821051904.0000     \n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 1s - loss: 371498713088.0000     \n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 1s - loss: 371176953173.3333     \n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 1s - loss: 370855848618.6667     \n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 1s - loss: 370535268352.0000     \n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 1s - loss: 370215387136.0000     \n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 1s - loss: 369895997440.0000     \n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 1s - loss: 369577044650.6667     \n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 1s - loss: 369258736298.6667     \n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 1s - loss: 368940744704.0000     \n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 1s - loss: 368623342933.3333     \n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 1s - loss: 368306356224.0000     \n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 1s - loss: 367989981184.0000     \n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 1s - loss: 367674217813.3333     \n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 1s - loss: 367358825813.3333     \n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 1s - loss: 367043958101.3333     \n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 1s - loss: 366729581909.3333     \n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 1s - loss: 366415642624.0000     \n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 1s - loss: 366102063786.6667     \n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 1s - loss: 365788910933.3333     \n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 1s - loss: 365476118528.0000     \n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 1s - loss: 365163850410.6667     \n",
      "Epoch 81/200\n",
      "3/3 [==============================] - 1s - loss: 364852062890.6667     \n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 1s - loss: 364540690432.0000     \n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 1s - loss: 364229765802.6667     \n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 1s - loss: 363919179776.0000     \n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 1s - loss: 363609139882.6667     \n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 1s - loss: 363299547818.6667     \n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 1s - loss: 362990338048.0000     \n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 1s - loss: 362681477802.6667     \n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 1s - loss: 362372923392.0000     \n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 1s - loss: 362064860501.3333     \n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 1s - loss: 361757125290.6667     \n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 1s - loss: 361449772373.3333     \n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 1s - loss: 361142790826.6667     \n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 1s - loss: 360836224341.3333     \n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 1s - loss: 360529941845.3333     \n",
      "Epoch 96/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s - loss: 360224041642.6667     \n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 1s - loss: 359918501888.0000     \n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 1s - loss: 359613377194.6667     \n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 1s - loss: 359308634794.6667     \n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 1s - loss: 359004132693.3333     \n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 1s - loss: 358700056576.0000     \n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 1s - loss: 358396319061.3333     \n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 1s - loss: 358092941994.6667     \n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 1s - loss: 357789925376.0000     \n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 1s - loss: 357487269205.3333     \n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 1s - loss: 357184842410.6667     \n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 1s - loss: 356882797909.3333     \n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 1s - loss: 356581059242.6667     \n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 1s - loss: 356279659178.6667     \n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 1s - loss: 355978608640.0000     \n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 1s - loss: 355677896704.0000     \n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 1s - loss: 355377523370.6667     \n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 1s - loss: 355077423104.0000     \n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 1s - loss: 354777672362.6667     \n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 1s - loss: 354478172842.6667     \n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 1s - loss: 354179022848.0000     \n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 1s - loss: 353880145920.0000     \n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 1s - loss: 353581574826.6667     \n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 1s - loss: 353283353258.6667     \n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 1s - loss: 352985404757.3333     \n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 1s - loss: 352687762090.6667     \n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 1s - loss: 352390414336.0000     \n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 1s - loss: 352093252266.6667     \n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 1s - loss: 351796363264.0000     \n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 1s - loss: 351499845632.0000     \n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 1s - loss: 351203622912.0000     \n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 1s - loss: 350907684181.3333     \n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 1s - loss: 350612084053.3333     \n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 1s - loss: 350316789760.0000     \n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 1s - loss: 350021757610.6667     \n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 1s - loss: 349727020373.3333     \n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 1s - loss: 349432501589.3333     \n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 1s - loss: 349138310485.3333     \n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 1s - loss: 348844392448.0000     \n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 1s - loss: 348550758400.0000     \n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 1s - loss: 348257408341.3333     \n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 1s - loss: 347964342272.0000     \n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 1s - loss: 347671429120.0000     \n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 1s - loss: 347378854570.6667     \n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 1s - loss: 347086553088.0000     \n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 2s - loss: 346794502826.6667     \n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 1s - loss: 346502769322.6667     \n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 1s - loss: 346211243349.3333     \n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 2s - loss: 345920012288.0000     \n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 1s - loss: 345628988757.3333     \n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 2s - loss: 345338216448.0000     \n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 1s - loss: 345047771818.6667     \n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 1s - loss: 344757643946.6667     \n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 1s - loss: 344467745450.6667     \n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 1s - loss: 344178087253.3333     \n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 1s - loss: 343888723968.0000     \n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 1s - loss: 343599611904.0000     \n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 1s - loss: 343310751061.3333     \n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 1s - loss: 343022196053.3333     \n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 1s - loss: 342733837653.3333     \n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 1s - loss: 342445697706.6667     \n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 1s - loss: 342157841749.3333     \n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 1s - loss: 341870182400.0000     \n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 1s - loss: 341582872576.0000     \n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 1s - loss: 341295813973.3333     \n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 1s - loss: 341008951978.6667     \n",
      "Epoch 162/200\n",
      "3/3 [==============================] - 1s - loss: 340722384896.0000     \n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 1s - loss: 340436058112.0000     \n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 1s - loss: 340150015317.3333     \n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 1s - loss: 339864190976.0000     \n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 1s - loss: 339578617856.0000     \n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 1s - loss: 339293285034.6667     \n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 1s - loss: 339008181589.3333     \n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 1s - loss: 338723318442.6667     \n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 1s - loss: 338438630058.6667     \n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 1s - loss: 338154269354.6667     \n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 1s - loss: 337870127104.0000     \n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 1s - loss: 337586214229.3333     \n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 1s - loss: 337302530730.6667     \n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 1s - loss: 337019142144.0000     \n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 1s - loss: 336735906474.6667     \n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 1s - loss: 336452922026.6667     \n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 1s - loss: 336170166954.6667     \n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 1s - loss: 335887674026.6667     \n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 1s - loss: 335605454165.3333     \n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 1s - loss: 335323452757.3333     \n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 1s - loss: 335041637034.6667     \n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 1s - loss: 334760094378.6667     \n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 1s - loss: 334478781098.6667     \n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 2s - loss: 334197664426.6667     \n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 1s - loss: 333916777130.6667     \n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 1s - loss: 333636108288.0000     \n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 1s - loss: 333355679744.0000     \n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 1s - loss: 333075502421.3333     \n",
      "Epoch 190/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s - loss: 332795598165.3333     \n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 1s - loss: 332515868672.0000     \n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 2s - loss: 332236313941.3333     \n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 1s - loss: 331957021354.6667     \n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 1s - loss: 331677990912.0000     \n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 1s - loss: 331399178922.6667     \n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 1s - loss: 331120552618.6667     \n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 1s - loss: 330842188458.6667     \n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 1s - loss: 330564053674.6667     \n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 1s - loss: 330286159189.3333     \n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 1s - loss: 330008461312.0000     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12c7f2748>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=200, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
